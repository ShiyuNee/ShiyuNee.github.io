---
permalink: /publications/
title: "Publications"
author_profile: true
redirect_from: 
  - /publications.html
---

## Core Works

> **LLM Knowledge Boundary Perception**

- **Towards Fully Exploiting LLM Internal States to Enhance Knowledge Boundary Perception**[[Arxiv](https://arxiv.org/abs/2502.11677)] [[Code](https://github.com/ShiyuNee/LLM-Knowledge-Boundary-Perception-via-Internal-States)] [[Poster](https://github.com/Trustworthy-Information-Access/LLM-Knowledge-Boundary-Perception-via-Internal-States/blob/master/poster.pdf)]<br><ins>**Shiyu Ni**</ins>, Keping Bi, Jiafeng Guo, Lulu Yu, Baolong Bi and Xueqi Cheng  <br>**ACL' 2025**:  The 63rd Annual Meeting of the Association for Computational Linguistics
- **When Do LLMs Need Retrieval Augmentation? Mitigating LLMs' Overconfidence Helps Retrieval Augmentation**[[Arxiv](https://arxiv.org/abs/2402.11457)] [[Blog](https://mp.weixin.qq.com/s/yhkGXXjYdoM-KIhHGgdjdA)] [[Code](https://github.com/ShiyuNee/When-to-Retrieve)]<br>
  <ins>**Shiyu Ni**</ins>, Keping Bi, Jiafeng Guo and Xueqi Cheng  <br>**ACL' 2024**:  Findings of the Association for Computational Linguistics, 2024
- **Are Large Language Models More Honest in Their Probabilistic or Verbalized Confidence?**[[Arxiv](https://arxiv.org/pdf/2408.09773)]<br>
  <ins>**Shiyu Ni**</ins>, Keping Bi, Lulu Yu and Jiafeng Guo  <br>**CCIR' 2024**: The 30th China Conference on Information Retrieval
- **Do LVLMs Know What They Know? A Systematic Study of Knowledge Boundary Perception in LVLMs**[[Arxiv](https://arxiv.org/abs/2508.19111)] [[Code](https://github.com/ShiyuNee/LVLM-Knowledge-Boundary-Perception)]<br>Zhikai Ding, <ins>**Shiyu Ni**</ins>, and Keping Bi  <br>**EMNLP' 2025**:  Findings of Empirical Methods in Natural Language Processing
- **How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary Perception**[[Arxiv](https://arxiv.org/pdf/2505.17537)]<br>
  <ins>**Shiyu Ni**</ins>, Keping Bi, Jiafeng Guo and Xueqi Cheng  <br>
- **Annotation-Efficient Universal Honesty Alignment**[[Arxiv](https://arxiv.org/abs/2510.17509)] [[Code](https://github.com/Trustworthy-Information-Access/Annotation-Efficient-Universal-Honesty-Alignment)]<br>
  <ins>**Shiyu Ni**</ins>, Keping Bi, Jiafeng Guo, Minghao Tang, Jingtong Wu, Zengxin Han and Xueqi Cheng  <br>

> **Clarifying Question Generation/Facet Generation**

- **A Comparative Study of Training Objectives for Clarification Facet Generation**[[PDF](https://arxiv.org/pdf/2310.00703v1.pdf)] [[Code](https://github.com/ShiyuNee/Facet-Generation)] [[PPT](https://github.com/ShiyuNee/Facet-Generation/blob/master/SIGIR-AP2023-Shiyu.pptx)] <br>
  <ins>**Shiyu Ni**</ins>, Keping Bi, Jiafeng Guo and Xueqi Cheng  <br>**SIGIR-AP' 2023**: Proceedings of the 1st International ACM SIGIR Conference on Information Retrieval in the Asia Pacific

> **Knowledge Utilization**

- **Injecting External Knowledge into the Reasoning Process Enhances Retrieval-Augmented Generation**[[Arxiv](https://www.arxiv.org/abs/2507.19333)] [[Code](https://github.com/mh-tang/Passage-Injection)]<br>Minghao Tang, <ins>**Shiyu Ni**</ins>, Jiafeng Guo and Keping Bi  <br>**SIGIR-AP' 2025**: Proceedings of the 3rd International ACM SIGIR Conference on Information Retrieval in the Asia Pacific
- **The Role of Parametric Injection-A Systematic Study of Parametric Retrieval-Augmented Generation**[[Arxiv](https://www.arxiv.org/abs/2510.12668)]<br>Minghao Tang, <ins>**Shiyu Ni**</ins>, Jingtong Wu, Zengxin Han and Keping Bi  <br>

## Collaborations

- **Is Factuality Enhancement a Free Lunch For LLMs? Better Factuality Can Lead to Worse Context-Faithfulness**[[Arxiv](https://arxiv.org/abs/2404.00216)]  <br>Baolong Bi, Shenghua Liu, Yiwei Wang, Lingrui Mei, Junfeng Fang, Hongcheng Gao, <ins>**Shiyu Ni**</ins> and Xueqi Cheng  <br>**ICLR' 2025**:  The Thirteenth International Conference on Learning Representations







